{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Develop Deep learning Model part 2 & 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2 - Defining the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature Extractor Model\n",
    "The input to this model is a vector of length 4096 obtained from the VGG model. We use 50% dropout at the input layer (to avoid over-fitting). Now the output layer of this model contains 256 units and uses 'relu' as the activation function in each output unit.\n",
    "\n",
    "#### Sequence Model\n",
    "The input to this model is a vector of length equal to the maximum lengths of the descriptors present. This input is passed through as Embedding layer and then next a 50% dropout is used after the Embedding layer. The output layer uses LSTM and has 256 units as output.\n",
    "\n",
    "#### Decoder Model\n",
    "The decoder model takes an input vector of length 256. This input is obtained by the addition of the outputs obtained at the output layers of the above two models. So the first layer is a dense layer with 256 units and uses 'relu' activation function. The output of this layer is again fed to another dense layer which uses 'softmax' activation function. The output is one hot encoded therefore the output layer will contain number of units equal to the vocabulary size.\n",
    "So the output we need is the output of the decoder mode;\n",
    "\n",
    "#### Using Model Class API\n",
    "The Model class API is used to instantiate a model. Given some input tensors(s) (in our case the extracted features from the photos and the descriptions are the input tensors) and output tensor(s) (in our case the output of the decoder).\n",
    "This model by itself will include all layers required in the computation of b given a and hence create a new model for you and return it.\n",
    "In the case of multi-input or multi-output models, you can use lists as well (as we will use for inputs since we have multiple(two) inputs):\n",
    "model = Model(inputs=[a1, a2], outputs=[b1, b2, b3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "define_model() defines and returns the model ready to be fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the captioning model\n",
    "def define_model(vocab_size, max_length):\n",
    "    \n",
    "    # feature extractor model\n",
    "    inputs1 = Input(shape=(4096,))\n",
    "    fe1 = Dropout(0.5)(inputs1)\n",
    "    fe2 = Dense(256, activation='relu')(fe1)\n",
    "    \n",
    "    # sequence model\n",
    "    inputs2 = Input(shape=(max_length,))\n",
    "    se1 = Embedding(vocab_size, 256, mask_zero=True)(inputs2)\n",
    "    se2 = Dropout(0.5)(se1)\n",
    "    se3 = LSTM(256)(se2)\n",
    "    \n",
    "    # decoder model\n",
    "    decoder1 = add([fe2, se3])\n",
    "    decoder2 = Dense(256, activation='relu')(decoder1)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder2)\n",
    "    \n",
    "    # tie it together [image, seq] [word]\n",
    "    model = Model(inputs=[inputs1, inputs2], outputs=outputs)\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    \n",
    "    # summarize model\n",
    "    print(model.summary())\n",
    "    plot_model(model, to_file='model.png', show_shapes=True)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3 - Fitting the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# define the model\n",
    "model = define_model(vocab_size, max_length)\n",
    "# define checkpoint callback\n",
    "filepath = 'model-ep{epoch:03d}-loss{loss:.3f}-val_loss{val_loss:.3f}.h5'\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "# fit model\n",
    "model.fit([X1train, X2train], ytrain, epochs=20, verbose=2, callbacks=[checkpoint], validation_data=([X1test, X2test], ytest))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
